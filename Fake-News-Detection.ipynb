{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Data\n",
    "df_train = pd.read_csv('Data/fake-news/train.csv').dropna()\n",
    "df_test = pd.read_csv('Data/fake-news/test.csv').dropna()\n",
    "df_all = pd.read_csv('Data/fake-news/submit.csv').dropna()\n",
    "\n",
    "# Checking training data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of TfidVectorizer to predict fake news is  61.2896174863388\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.69      0.63      2213\n",
      "           1       0.65      0.54      0.59      2362\n",
      "\n",
      "    accuracy                           0.61      4575\n",
      "   macro avg       0.62      0.62      0.61      4575\n",
      "weighted avg       0.62      0.61      0.61      4575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhishekjain/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" The code here is to apply PassiveAggresive Classifier \"\"\"\n",
    "# Assigning the train and test data\n",
    "X_train = df_train['text']\n",
    "# X_train = df_train.drop(['id', 'title', 'author', 'label'], axis=1)\n",
    "Y_train = df_train.drop(['id', 'title', 'author', 'text'], axis=1)\n",
    "#X_test = df_test.drop(['id', 'title', 'author'], axis=1)\n",
    "X_test = df_test['text']\n",
    "\n",
    "# Creating an empty dataframe\n",
    "#df_y_test = pd.DataFrame(columns=['Y_test']) - This creates new DF but appending is time consuming\n",
    "\n",
    "df_y_test = df_test.drop(['title', 'author', 'text'], axis=1) # Creates a new DF with 'ID' same as testDF\n",
    "\n",
    "# df_y_test['labels'] = np.zeros((len(df_test), 1)) - Another way is to create a DF of zeros of size same as test DF\n",
    "\n",
    "# Extracting y data of X_test from submit df\n",
    "\n",
    "# Method 1 - Iterating using for loop - takes a lot of time to iterate over 20k rows\n",
    "# for i, j in df_all.iterrows():\n",
    "#     for k, l in df_test.iterrows():\n",
    "#         if j['id'] == l['id']:\n",
    "#             df_y_test.loc[j['id'], 'label'] = j['label']\n",
    "\n",
    "# Use Method 2 or 3 with merge on pandas DF and get the needed DF\n",
    "# Method 2\n",
    "df_y_test = df_y_test.merge(df_all, left_on='id', right_on='id', how='left')[['id', 'label']]\n",
    "\n",
    "#Method 3\n",
    "# ytestdf = pd.merge(left = ytestdf, right=alldf, left_on='id', right_on='id', how='left')\n",
    "\n",
    "#Y_test = df_y_test.drop(['id'], axis=1)\n",
    "Y_test = df_y_test['label']\n",
    "\n",
    "# Initializing TfidVectoriser\n",
    "tfidf_vector = TfidfVectorizer(stop_words='english', max_df = 0.5)\n",
    "TVX_train = tfidf_vector.fit_transform(X_train)\n",
    "TVX_test = tfidf_vector.transform(X_test)\n",
    "\n",
    "#Initializing PassiveAggresive Classifier\n",
    "PAClass = PassiveAggressiveClassifier(max_iter=5000)\n",
    "\n",
    "# Fit and Tranform the training data\n",
    "PAClass.fit(TVX_train, Y_train)\n",
    "Y_pred = PAClass.predict(TVX_test)\n",
    "\n",
    "# Checking the accuracy, classification report and printing them\n",
    "FK_accuracy = accuracy_score(Y_test, Y_pred) * 100\n",
    "FK_classification_report = classification_report(Y_test, Y_pred)\n",
    "\n",
    "print('The accuracy of TfidVectorizer to predict fake news is ', FK_accuracy)\n",
    "print('Classification Report: \\n', FK_classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/abhishekjain/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/24 20:07:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 20:07:48 WARN TaskSetManager: Stage 0 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------------+--------------------+-----+\n",
      "| id|               title|            author|                text|label|\n",
      "+---+--------------------+------------------+--------------------+-----+\n",
      "|  0|House Dem Aide: W...|     Darrell Lucus|House Dem Aide: W...|    1|\n",
      "|  1|FLYNN: Hillary Cl...|   Daniel J. Flynn|Ever get the feel...|    0|\n",
      "|  2|Why the Truth Mig...|Consortiumnews.com|Why the Truth Mig...|    1|\n",
      "|  3|15 Civilians Kill...|   Jessica Purkiss|Videos 15 Civilia...|    1|\n",
      "|  4|Iranian woman jai...|    Howard Portnoy|Print \\nAn Irania...|    1|\n",
      "+---+--------------------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Starting a Spark Session\n",
    "fk_spark = SparkSession.builder.appName('FakeNewsDetection').getOrCreate()\n",
    "        \n",
    "# Creating Spark Dataframe\n",
    "df_train_spark = fk_spark.createDataFrame(df_train.astype(str)) # We can either use .astype(str) which is in Pandas or use StructField to create Schema\n",
    "df_test_spark = fk_spark.createDataFrame(df_test.astype(str))\n",
    "\n",
    "#Checking training data\n",
    "df_train_spark.printSchema()\n",
    "df_train_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing GBTClassifier - Long way\n",
    "\n",
    "# # Removing punctuations\n",
    "# regex = '[,\\\\-]'\n",
    "# df_train_spark = df_train_spark.withColumn('text', regexp_replace(df_train_spark.text, regex))\n",
    "# df_test_spark = df_test_spark.withColumn('text', regexp_replace(df_test_spark.text, regex))\n",
    "\n",
    "# # Turning texts to tokens\n",
    "# df_train_spark = Tokenizer(inputCol='text', outputCol='tokens').transform(df_train_spark)\n",
    "# df_test_spark = Tokenizer(inputCol='text', outputCol='tokens').transform(df_test_spark)\n",
    "\n",
    "# # Removing stop words such as I, me, him, her etc.\n",
    "# df_train_spark = StopWordsRemover(inputCol='tokens', outputCol='words').transform(df_train_spark)\n",
    "# df_test_spark = StopWordsRemover(inputCol='tokens', outputCol='words').transform(df_test_spark)\n",
    "\n",
    "# # Hashing features or converting words to numbers\n",
    "# df_train_spark = HashingTF(inputCol='words', outputCol='hashed', numFeatures=32).transform(df_train_spark)\n",
    "# df_test_spark = HashingTF(inputCol='words', outputCol='hashed', numFeatures=32).transform(df_test_spark)\n",
    "\n",
    "# # Converting hashed to TF-IDF or Dealing with really common and most frequent words\n",
    "# tfidf_train = IDF(inputCol='hashed', outputCol='features').fit(df_train_spark).transform(df_train_spark)\n",
    "# tfidf_test = IDF(inputCol='hashed', outputCol='features').fit(df_test_spark).transform(df_test_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 21:06:37 WARN TaskSetManager: Stage 522 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:40 WARN TaskSetManager: Stage 524 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:40 WARN TaskSetManager: Stage 525 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:43 WARN TaskSetManager: Stage 526 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:46 WARN TaskSetManager: Stage 528 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 530 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 532 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 534 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 536 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 538 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 540 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 542 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 544 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/03/24 21:06:49 WARN TaskSetManager: Stage 546 contains a task of very large size (10267 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Creating label Dataframe for the Test data\n",
    "Y_sparktest = fk_spark.createDataFrame(df_y_test.astype(str))\n",
    "\n",
    "# Implementing GBT Classifier using Pipeline\n",
    "\n",
    "# Removing punctuations\n",
    "regex = '[,\\\\-]'\n",
    "df_train_spark = df_train_spark.withColumn('text', regexp_replace(df_train_spark.text, regex, ' '))\n",
    "df_test_spark = df_test_spark.withColumn('text', regexp_replace(df_test_spark.text, regex, ' '))\n",
    "\n",
    "# Turning text to tokens\n",
    "Text_token = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "\n",
    "# Removing stop words such as I, me, him, her etc.\n",
    "RemStopWords = StopWordsRemover(inputCol='tokens', outputCol='words')\n",
    "\n",
    "# Hashing features or converting words to numbers\n",
    "TextHash = HashingTF(inputCol='words', outputCol='hashed', numFeatures=32)\n",
    "\n",
    "# Converting hashed to TF-IDF or Dealing with really common and most frequent words\n",
    "HashIDF = IDF(inputCol='hashed', outputCol='features')\n",
    "\n",
    "# Initiating GBT Classifier\n",
    "classifier = GBTClassifier(maxIter=1)\n",
    "\n",
    "# Creating pipeline\n",
    "pipeline = Pipeline(stages=[Text_token, RemStopWords, TextHash, HashIDF, classifier])\n",
    "\n",
    "# Implementing Pipeline on Training Data\n",
    "pipeline = pipeline.fit(df_train_spark.select(col('text'), col('label').cast('int')))\n",
    "\n",
    "# Making predictions using Test Data\n",
    "predictions = pipeline.transform(df_test_spark.select(col('text'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/24 21:51:29 WARN TaskSetManager: Stage 551 contains a task of very large size (2805 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4575,)\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "The accuracy of GBT Classifier to predict fake news is  66.81967213114754\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.81      0.70      2213\n",
      "           1       0.75      0.54      0.62      2362\n",
      "\n",
      "    accuracy                           0.67      4575\n",
      "   macro avg       0.69      0.67      0.66      4575\n",
      "weighted avg       0.69      0.67      0.66      4575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the accuracy, classification report of GBT Classifier and printing them\n",
    "# Converting predictions to pandas df\n",
    "Yspark_pred = predictions.toPandas()\n",
    "Yspark_pred = Yspark_pred['prediction']\n",
    "\n",
    "FK_accuracy = accuracy_score(Y_test, Yspark_pred) * 100\n",
    "FK_classification_report = classification_report(Y_test, Yspark_pred)\n",
    "\n",
    "print('The accuracy of GBT Classifier to predict fake news is ', FK_accuracy)\n",
    "print('Classification Report: \\n', FK_classification_report)\n",
    "\n",
    "# Stopping the spark session\n",
    "fk_spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
